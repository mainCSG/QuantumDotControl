{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mask R-CNN : Train on a Charge Stability Diagram Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import math\n",
    "import re\n",
    "import time\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "\n",
    "import keras\n",
    "# import pandas as pd\n",
    "import skimage as sk\n",
    "import scipy as sp\n",
    "import tensorflow as tf\n",
    "import platform\n",
    "\n",
    "# from data.process_data import *\n",
    "\n",
    "# Root directory of the project\n",
    "ROOT_DIR = os.path.abspath(\".\")\n",
    "print(ROOT_DIR)\n",
    "\n",
    "# Import Mask RCNN\n",
    "sys.path.append(ROOT_DIR)  # To find local version of the library\n",
    "from mrcnn.config import Config\n",
    "from mrcnn import utils\n",
    "import mrcnn.model as modellib\n",
    "from mrcnn import visualize\n",
    "from mrcnn.model import log\n",
    "\n",
    "%matplotlib inline \n",
    "\n",
    "# Directory to save logs and trained model\n",
    "MODEL_DIR = os.path.join(ROOT_DIR, \"logs\")\n",
    "\n",
    "# Local path to trained weights file\n",
    "COCO_MODEL_PATH = os.path.join(ROOT_DIR, \"mask_rcnn_coco.h5\")\n",
    "# Download COCO trained weights from Releases if needed\n",
    "if not os.path.exists(COCO_MODEL_PATH):\n",
    "    utils.download_trained_weights(COCO_MODEL_PATH)\n",
    "\n",
    "print(f\"Python Platform: {platform.platform()}\")\n",
    "print(f\"Tensor Flow Version: {tf.__version__}\")\n",
    "print(f\"Keras Version: {keras.__version__}\")\n",
    "print()\n",
    "print(f\"Python {sys.version}\")\n",
    "print(f\"Scikit-Learn {sk.__version__}\")\n",
    "print(f\"SciPy {sp.__version__}\")\n",
    "gpu = len(tf.config.list_physical_devices('GPU'))>0\n",
    "print(\"GPU is\", \"available\" if gpu else \"NOT AVAILABLE\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class qFlowConfig(Config):\n",
    "    \"\"\"Configuration for training on the qFlow dataset.\n",
    "    Derives from the base Config class and overrides values specific\n",
    "    to the qFlow dataset.\n",
    "    \"\"\"\n",
    "    # Give the configuration a recognizable name\n",
    "    NAME = \"qFlow\"\n",
    "\n",
    "    # Train on 1 GPU and 8 images per GPU. We can put multiple images on each\n",
    "    # GPU because the images are small. Batch size is 8 (GPUs * images/GPU).\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 4\n",
    "    \n",
    "    # Number of classes (including background)\n",
    "    NUM_CLASSES = 1 + 3  # background + 3 regimes (0, 1, 2 QD)\n",
    "\n",
    "    # Use small images for faster training. Set the limits of the small side\n",
    "    # the large side, and that determines the image shape.\n",
    "    IMAGE_MIN_DIM = 128\n",
    "    IMAGE_MAX_DIM = 128\n",
    "\n",
    "    # Use smaller anchors because our image and objects are small\n",
    "    RPN_ANCHOR_SCALES = (8, 16, 32, 64, 128)  # anchor side in pixels\n",
    "\n",
    "    # Reduce training ROIs per image because the images are small and have\n",
    "    # few objects. Aim to allow ROI sampling to pick 33% positive ROIs.\n",
    "    TRAIN_ROIS_PER_IMAGE = 32\n",
    "\n",
    "    # Use a small epoch since the data is simple\n",
    "    STEPS_PER_EPOCH = 100\n",
    "\n",
    "    # use small validation steps since the epoch is small\n",
    "    VALIDATION_STEPS = 5\n",
    "    \n",
    "config = qFlowConfig()\n",
    "config.display()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "\n",
    "Load in qFlow data.\n",
    "\n",
    "Extend the Dataset class and add a method to load the shapes dataset, `load_shapes()`, and override the following methods:\n",
    "\n",
    "* load_image()\n",
    "* load_mask()\n",
    "* image_reference()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load CSD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NumpyEncoder(json.JSONEncoder):\n",
    "    \"\"\" Special json encoder for numpy types \"\"\"\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.integer):\n",
    "            return int(obj)\n",
    "        elif isinstance(obj, np.floating):\n",
    "            return float(obj)\n",
    "        elif isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        return json.JSONEncoder.default(self, obj)\n",
    "\n",
    "class ProcessData():\n",
    "    def __init__(self):\n",
    "        self.vgg_format = {}\n",
    "\n",
    "    def extract_csd(self, path_to_data):\n",
    "        qflow_data = np.load(path_to_data, allow_pickle=True).item()\n",
    "\n",
    "        voltages = {\"P1\": qflow_data['V_P1_vec'], \"P2\": qflow_data['V_P2_vec']}\n",
    "        N = len(voltages[\"P1\"])\n",
    "        M = len(voltages[\"P2\"])\n",
    "        \n",
    "        assert N == M\n",
    "\n",
    "        self.width, self.height = N,M\n",
    "        self.fileID = os.path.basename(path_to_data).replace(\".npy\",\"\")\n",
    "        self.filename = self.fileID+\".npy\"\n",
    "\n",
    "        # Extract current, state regimes and gradient\n",
    "        I = np.array([\n",
    "            data['current'] for data in qflow_data['output']\n",
    "        ]).reshape((N,N))\n",
    "\n",
    "        regimes = np.array([\n",
    "            data['state'] for data in qflow_data['output']\n",
    "        ]).reshape((N,N))\n",
    "\n",
    "        def normalize(matrix):\n",
    "            mean = matrix.mean()\n",
    "            std = matrix.std()\n",
    "            return (matrix - mean) / std\n",
    "\n",
    "        I = normalize(I)\n",
    "\n",
    "        csd = {\"I\": np.array(I,dtype=np.float32), \"regimes\": regimes}\n",
    "\n",
    "        return csd\n",
    "    \n",
    "    def vgg_annotate_csd(self,csd):\n",
    "\n",
    "        # First create a list of objects in the image\n",
    "        object_list = []\n",
    "\n",
    "        regimes = csd[\"regimes\"]\n",
    "\n",
    "        labelled_regimes = sk.measure.label(\n",
    "            regimes, background = -1, connectivity = 1\n",
    "        )\n",
    "\n",
    "        regions = sk.measure.regionprops(labelled_regimes)\n",
    "\n",
    "        regions_list = []\n",
    "        for index in range(1, labelled_regimes.max()):\n",
    "                region_dict = {}\n",
    "                vertices = regions[index].coords\n",
    "                y, x = vertices.T\n",
    "\n",
    "                regime = regimes[int(np.average(y)), int(np.average(x))]\n",
    "                region_dict[\"shape_name\"] = \"polygon\"\n",
    "                region_dict[\"all_points_x\"] = x.tolist()\n",
    "                region_dict[\"all_points_y\"] = y.tolist()\n",
    "                region_dict[\"class\"] = regime\n",
    "\n",
    "                regions_list.append(region_dict)\n",
    "        \n",
    "        object = {}\n",
    "        object[\"filename\"] = self.filename\n",
    "        object[\"size\"] = self.width * self.height\n",
    "        object[\"regions\"] = regions_list\n",
    "\n",
    "        object_list.append(object)\n",
    "\n",
    "        # Now create the VGG format\n",
    "        \n",
    "        for object in object_list:\n",
    "             \n",
    "            filename = object[\"filename\"]\n",
    "            size = object[\"size\"]\n",
    "            regions = object[\"regions\"]\n",
    "\n",
    "            self.vgg_format[self.fileID] = {}\n",
    "            self.vgg_format[self.fileID][\"filename\"] = filename\n",
    "            self.vgg_format[self.fileID][\"size\"] = size\n",
    "            self.vgg_format[self.fileID][\"regions\"] = {}\n",
    "\n",
    "            index = 0\n",
    "\n",
    "            for region in regions:\n",
    "                region_dict = {}\n",
    "                shape_attributes = {}\n",
    "\n",
    "                shape_attributes[\"name\"] = region[\"shape_name\"]\n",
    "                shape_attributes[\"all_points_x\"] = region[\"all_points_x\"]\n",
    "                shape_attributes[\"all_points_y\"] = region[\"all_points_y\"]\n",
    "\n",
    "                region_dict[\"shape_attributes\"] = shape_attributes\n",
    "                region_dict[\"region_attributes\"] = {\"label\": region[\"class\"]}\n",
    "\n",
    "                self.vgg_format[self.fileID][\"regions\"][str(index)] = region_dict\n",
    "\n",
    "                index += 1\n",
    "\n",
    "        self.vgg_format[self.fileID][\"file_attributes\"] = {}\n",
    "\n",
    "    def dump_json(self,train_val_split, dataset_size, save_folder):\n",
    "            num_of_train = int(train_val_split * dataset_size)\n",
    "\n",
    "            keys, values = zip(*self.vgg_format.items())\n",
    "            vgg_format_train = dict(zip(keys[:num_of_train], values[:num_of_train]))\n",
    "            vgg_format_val = dict(zip(keys[num_of_train:], values[num_of_train:]))\n",
    "\n",
    "            with open(save_folder+\"/train/annotations_vgg.json\", \"w\") as f:\n",
    "                json.dump(vgg_format_train, f, cls=NumpyEncoder)\n",
    "            with open(save_folder+\"/val/annotations_vgg.json\", \"w\") as f:\n",
    "                json.dump(vgg_format_val, f, cls=NumpyEncoder)\n",
    "\n",
    "\n",
    "ProcessData = ProcessData()\n",
    "datapath = \"/Users/andrijapaurevic/Documents/uWaterloo/research/mainCSG/QuantumDotControl/data/raw/\"\n",
    "save_dir = \"/Users/andrijapaurevic/Documents/uWaterloo/research/mainCSG/QuantumDotControl/autotuning/coarse_tuning/models/mask_rcnn\"\n",
    "dataset_size = 50\n",
    "train_val_split = 0.8\n",
    "counter = 0\n",
    "\n",
    "dataset = []\n",
    "for filename in os.listdir(datapath):\n",
    "    if filename.endswith(\".npy\"):\n",
    "        if counter == dataset_size: \n",
    "            break\n",
    "\n",
    "        csd = ProcessData.extract_csd(os.path.join(datapath,filename))\n",
    "        ProcessData.vgg_annotate_csd(csd)\n",
    "\n",
    "        dataset.append(csd)\n",
    "\n",
    "        counter += 1\n",
    "\n",
    "ProcessData.dump_json(train_val_split, dataset_size, save_dir)\n",
    "\n",
    "# plt.imshow(dataset[0][\"I\"])\n",
    "# plt.title(\"Sample CSD\")\n",
    "# plt.show()\n",
    "\n",
    "# plt.imshow(dataset[0][\"regimes\"])\n",
    "# plt.title(\"Sample CSD\")\n",
    "# plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Custom Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(utils.Dataset):\n",
    "    def __init__(self):\n",
    "        super(CustomDataset, self).__init__()\n",
    "\n",
    "    def load_custom(self, annotations_dir: str, dataset_dir: str, suffix: str):\n",
    "        assert suffix in [\"train\", \"val\"]\n",
    "\n",
    "        self.add_class(\"custom\", 0, \"0 QD\")\n",
    "        self.add_class(\"custom\", 1, \"1 QD\")\n",
    "        self.add_class(\"custom\", 2, \"2 QD\")\n",
    "\n",
    "        annotations = json.load(\n",
    "            open(\n",
    "                os.path.join(annotations_dir, f\"{suffix}/annotations_vgg.json\")\n",
    "            )\n",
    "        )\n",
    "        annotations = list(annotations.values()) # VIA 2.0 expects list not dict\n",
    "        annotations = [a for a in annotations if a[\"regions\"]] # Skip unannotated images\n",
    "        \n",
    "        for a in annotations:\n",
    "            polygons = [region['shape_attributes'] for region in a['regions'].values()]\n",
    "            custom = [region['region_attributes'] for region in a['regions'].values()]\n",
    "\n",
    "            num_ids = []\n",
    "            for n in custom:\n",
    "                try:\n",
    "                    if n['label'] == 0:\n",
    "                        num_ids.append(1)\n",
    "                    elif n['label'] == 1:\n",
    "                        num_ids.append(2)\n",
    "                    elif n['label'] == 2:\n",
    "                        num_ids.append(3)\n",
    "                except:\n",
    "                    pass\n",
    "                        \n",
    "            image_path = os.path.join(dataset_dir, a['filename'])\n",
    "\n",
    "            height, width = int(np.sqrt(a['size'])), int(np.sqrt(a['size']))\n",
    "\n",
    "            self.add_image(\n",
    "                \"custom\",\n",
    "                image_id=a['filename'],\n",
    "                path = image_path,\n",
    "                width = width,\n",
    "                height = height,\n",
    "                polygons = polygons,\n",
    "                num_ids = num_ids\n",
    "            )\n",
    "\n",
    "    def load_mask(self, image_id):\n",
    "\n",
    "        image_info = self.image_info[image_id]\n",
    "        if image_info[\"source\"] != \"custom\":\n",
    "            return super(self.__class__, self).load_mask(image_id)\n",
    "        num_ids = image_info['num_ids']\n",
    "\n",
    "        mask = np.zeros(\n",
    "            [image_info[\"height\"], image_info['width'], len(image_info['polygons'])], dtype=np.uint8\n",
    "        )    \n",
    "\n",
    "        for i, p in enumerate(image_info[\"polygons\"]):\n",
    "            rr, cc = sk.draw.polygon(p['all_points_y'], p['all_points_x'])\n",
    "            mask[rr,cc,i] = 1\n",
    "\n",
    "        num_ids = np.array(num_ids, dtype=np.int32)\n",
    "\n",
    "        return mask, num_ids\n",
    "\n",
    "\n",
    "    def image_reference(self, image_id):\n",
    "        info = self.image_info[image_id]\n",
    "        if info[\"source\"] == \"custom\":\n",
    "            return info[\"path\"]\n",
    "        return super(self.__class__,self).image_reference(image_id)\n",
    "dataset_train = CustomDataset()\n",
    "dataset_train.load_custom(ROOT_DIR,datapath, \"train\")\n",
    "dataset_train.prepare()\n",
    "\n",
    "dataset_val = CustomDataset()\n",
    "dataset_val.load_custom(ROOT_DIR,datapath,\"val\")\n",
    "dataset_val.prepare()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model in training mode\n",
    "model = modellib.MaskRCNN(mode=\"training\", config=config,\n",
    "                          model_dir=MODEL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which weights to start with?\n",
    "init_with = \"coco\"  # imagenet, coco, or last\n",
    "\n",
    "if init_with == \"imagenet\":\n",
    "    model.load_weights(model.get_imagenet_weights(), by_name=True)\n",
    "elif init_with == \"coco\":\n",
    "    # Load weights trained on MS COCO, but skip layers that\n",
    "    # are different due to the different number of classes\n",
    "    # See README for instructions to download the COCO weights\n",
    "    model.load_weights(COCO_MODEL_PATH, by_name=True,\n",
    "                       exclude=[\"mrcnn_class_logits\", \"mrcnn_bbox_fc\", \n",
    "                                \"mrcnn_bbox\", \"mrcnn_mask\"])\n",
    "elif init_with == \"last\":\n",
    "    # Load the last model you trained and continue training\n",
    "    model.load_weights(model.find_last(), by_name=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the head branches\n",
    "# Passing layers=\"heads\" freezes all layers except the head\n",
    "# layers. You can also pass a regular expression to select\n",
    "# which layers to train by name pattern.\n",
    "model.train(dataset_train, dataset_val, \n",
    "            learning_rate=config.LEARNING_RATE, \n",
    "            epochs=1, \n",
    "            layers='heads')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine tune all layers\n",
    "# Passing layers=\"all\" trains all layers. You can also \n",
    "# pass a regular expression to select which layers to\n",
    "# train by name pattern.\n",
    "model.train(dataset_train, dataset_val, \n",
    "            learning_rate=config.LEARNING_RATE / 10,\n",
    "            epochs=2, \n",
    "            layers=\"all\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InferenceConfig(qFlowConfig):\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 1\n",
    "\n",
    "inference_config = InferenceConfig()\n",
    "\n",
    "# Recreate the model in inference mode\n",
    "model = modellib.MaskRCNN(mode=\"inference\", \n",
    "                          config=inference_config,\n",
    "                          model_dir=MODEL_DIR)\n",
    "\n",
    "# Get path to saved weights\n",
    "# Either set a specific path or find last trained weights\n",
    "# model_path = os.path.join(ROOT_DIR, \".h5 file name here\")\n",
    "model_path = model.find_last()\n",
    "\n",
    "# Load trained weights\n",
    "print(\"Loading weights from \", model_path)\n",
    "model.load_weights(model_path, by_name=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on a random image\n",
    "image_id = random.choice(dataset_val.image_ids)\n",
    "original_image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n",
    "    modellib.load_image_gt(dataset_val, inference_config, \n",
    "                           image_id, use_mini_mask=False)\n",
    "\n",
    "log(\"original_image\", original_image)\n",
    "log(\"image_meta\", image_meta)\n",
    "log(\"gt_class_id\", gt_class_id)\n",
    "log(\"gt_bbox\", gt_bbox)\n",
    "log(\"gt_mask\", gt_mask)\n",
    "\n",
    "visualize.display_instances(original_image, gt_bbox, gt_mask, gt_class_id, \n",
    "                            dataset_val.class_names, figsize=(8, 8))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute VOC-Style mAP @ IoU=0.5\n",
    "# Running on 10 images. Increase for better accuracy.\n",
    "image_ids = np.random.choice(dataset_val.image_ids, 10)\n",
    "APs = []\n",
    "for image_id in image_ids:\n",
    "    # Load image and ground truth data\n",
    "    image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n",
    "        modellib.load_image_gt(dataset_val, inference_config,\n",
    "                               image_id, use_mini_mask=False)\n",
    "    molded_images = np.expand_dims(modellib.mold_image(image, inference_config), 0)\n",
    "    # Run object detection\n",
    "    results = model.detect([image], verbose=0)\n",
    "\n",
    "    r = results[0]\n",
    "    # Compute AP\n",
    "    AP, precisions, recalls, overlaps =\\\n",
    "        utils.compute_ap(gt_bbox, gt_class_id, gt_mask,\n",
    "                         r[\"rois\"], r[\"class_ids\"], r[\"scores\"], r['masks'])\n",
    "    APs.append(AP)\n",
    "\n",
    "visualize.display_differences(\n",
    "image,\n",
    "gt_bbox, gt_class_id, gt_mask,\n",
    "r['rois'], r['class_ids'], r['scores'], r['masks'],\n",
    "dataset_val.class_names,\n",
    "show_box=False, show_mask=False,\n",
    "iou_threshold=0.5, score_threshold=0.5)\n",
    "    \n",
    "print(\"mAP: \", np.mean(APs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
